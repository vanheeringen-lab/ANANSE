{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from gimmemotifs.motif import read_motifs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, minmax_scale\n",
    "from glob import glob\n",
    "import random\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import qnorm\n",
    "import re\n",
    "\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models\n",
    "\n",
    "Warning: takes a lot of memory! It is not optimized for performance right now.\n",
    "\n",
    "Required layout:\n",
    "\n",
    "```\n",
    "train_dir/\n",
    "    peaks/\n",
    "        TF1.cell_type1.narrowPeak\n",
    "        TF2.cell_type1.narrowPeak\n",
    "        TF1.cell_type2.narrowPeak\n",
    "    remap_overlap/\n",
    "    {date}_trained/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config: files and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = \"/bheuts/ANANSE/hg19/model_training/ANANSE-CAGE\"\n",
    "\n",
    "# Directory for training data\n",
    "train_dir = f\"{BASEDIR}\"  # for layout see above\n",
    "\n",
    "\n",
    "# Output directory for models\n",
    "out_dir = f\"{BASEDIR}/2022-05-04_trained\"\n",
    "\n",
    "# Output directory for models\n",
    "out_overlap = f\"{BASEDIR}/remap_overlap\"\n",
    "\n",
    "    \n",
    "# CAGE cell line data\n",
    "ref_bed = f\"{BASEDIR}/CAGE.enhancers.bed\"\n",
    "\n",
    "# Results of motif scan, code does currently not include generating this:\n",
    "# !gimme scan {ref_bed} -g \"/ceph/rimlsfnwi/data/molbio/martens/bheuts/ANANSE/genome/hg19/hg19.fa\" -T > {BASEDIR}/CAGE.enhancers.motifs.txt\n",
    "motif_scan_file = f\"{BASEDIR}/CAGE.enhancers.motifs.txt\" \n",
    "\n",
    "# Coverage file\n",
    "coverage_bw = f\"{BASEDIR}/remap2022.hg19.w50.bw\"\n",
    "\n",
    "#Cell type name mapping, to match cell types of CAGE to cell type from ReMap\n",
    "ct_map = {\n",
    "    \"HepG2\":\"Hep-G2\",\n",
    "    \"HELA\":\"HeLa-S3\", \n",
    "#     \"LNCAP\":\"LNCaP\",\n",
    "    \"H1-hESC\":\"hESC\", \n",
    "    \"K562\":\"K-562\",\n",
    "    \"H9-hESC\":\"hESC\", \n",
    "    \"HEPG2\": \"Hep-G2\", \n",
    "#     \"MCF7\": \"MCF-7\"\n",
    "}\n",
    "\n",
    "cell_types = [\"GM12878\"]  # extra cell types\n",
    "cell_types = set(cell_types + list(ct_map.values()))\n",
    "\n",
    "force_rerun = True\n",
    " \n",
    "# Remove old files\n",
    "if os.path.exists(f\"{out_dir}\"):\n",
    "    ! rm -r {out_dir}\n",
    "    ! mkdir {out_dir}\n",
    "    \n",
    "if os.path.exists(f\"{out_overlap}\"):\n",
    "    ! rm -r {out_overlap}\n",
    "    ! mkdir {out_overlap}\n",
    "    \n",
    "if not os.path.exists(out_overlap):\n",
    "    os.makedirs(out_overlap)\n",
    "    \n",
    "if os.path.exists(f\"{train_dir}/all_tfs_y_true.feather\"):\n",
    "    ! rm {train_dir}/all_tfs_y_true.feather\n",
    "    \n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config: models and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "# model = RandomForestClassifier(n_jobs=-1, n_estimators=500, class_weight=\"balanced\")\n",
    "model = LogisticRegressionCV(class_weight=\"balanced\", n_jobs=24, )\n",
    "\n",
    "# Evaluation\n",
    "scores = {\"pr_auc\":average_precision_score, \"roc_auc\":roc_auc_score}\n",
    "\n",
    "# Specify test chromosome by regex\n",
    "test_chrom_regex = \"chr(1|8|21)[^\\d]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAGE data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAGEfightR bidirectional sites QC\n",
    "\n",
    "df = pd.read_table(f\"bidirectional.merged_headers.txt\")\n",
    "# print(df.head())\n",
    "\n",
    "df2 = df.copy()\n",
    "df2 = df2.set_index(\"Id\")\n",
    "for col in df2.columns.values:\n",
    "    df2[col] = df2[col] + 1\n",
    "    df2[col] = np.log10(df2[col])\n",
    "\n",
    "plt.hist(df2.sum(1), range=[0, 2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centre bidirectional regions and set a window of 200 bp\n",
    "\n",
    "regions = df[\"Id\"].str.split('[:-]', expand=True)\n",
    "regions.columns = [\"chrom\", \"start\", \"end\"]\n",
    "regions[\"start\"] = regions[\"start\"].astype(int)\n",
    "regions[\"end\"] = regions[\"end\"].astype(int)\n",
    "\n",
    "center = ((regions[\"start\"] + regions[\"end\"]) / 2).astype(int)\n",
    "regions[\"start\"] = center - 100\n",
    "regions[\"end\"] = center + 100\n",
    "\n",
    "regions.to_csv(ref_bed, header=False, index=False, sep=\"\\t\")\n",
    "\n",
    "loc = regions[\"chrom\"] + \":\" + regions[\"start\"].astype(str) + \"-\"  + regions[\"end\"].astype(str)\n",
    "\n",
    "df.index = loc\n",
    "df = df.drop(columns=\"Id\")\n",
    "data = df\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read motifs and factors\n",
    "\n",
    "Read a GimmeMotifs databases and load associated factors. Only factors that are TFs according to [Lovering et al. 2020](https://www.biorxiv.org/content/10.1101/2020.10.28.359232v2.full) are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_factors = pd.read_excel(\n",
    "    \"https://www.biorxiv.org/content/biorxiv/early/2020/12/07/2020.10.28.359232/DC1/embed/media-1.xlsx\",\n",
    "    engine='openpyxl', sheet_name=1)\n",
    "valid_factors = valid_factors.loc[valid_factors[\"Pseudogene\"].isnull(), \"HGNC approved gene symbol\"].values\n",
    "valid_factors = [f for f in valid_factors if f not in [\"EP300\", \"EZH2\"]]\n",
    "print(f\"{len(valid_factors)} TFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create overlap with TF (CAGE) peaks and ReMap enhancers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cell_type in cell_types:\n",
    "    for fname in glob(f\"{train_dir}/peaks/*.{cell_type}.narrowPeak\"):\n",
    "        tf, cell_type = fname.split(\"/\")[-1].split(\".\")[:2]\n",
    "        if not (ct_map.get(cell_type, cell_type) in cell_types and tf in valid_factors):\n",
    "            logger.debug(f\"skipping {tf} {cell_type}\")\n",
    "            continue\n",
    "  \n",
    "        if not os.path.exists(f\"{out_overlap}/{tf}.{cell_type}.enhancers.txt\"):\n",
    "            logger.debug(f\"converting {tf} {cell_type}\")\n",
    "            !bedtools intersect -a {ref_bed} -b {train_dir}/peaks/{tf}.{cell_type}.narrowPeak  -c > {out_overlap}/{tf}.{cell_type}.enhancers.bed\n",
    "            !cat {out_overlap}/{tf}.{cell_type}.enhancers.bed | sed 's/\\t/:/' | sed 's/\\t/-/' > {out_overlap}/{tf}.{cell_type}.enhancers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "motifs = read_motifs(as_dict=True)\n",
    "indirect = True\n",
    "f2m = {}\n",
    "for name, motif in motifs.items():\n",
    "    for k, factors in motif.factors.items():\n",
    "        if k != \"direct\" and not indirect:\n",
    "            print(\"skip\")\n",
    "            continue\n",
    "        for factor in factors:\n",
    "            f2m.setdefault(factor.upper(), []).append(name)\n",
    "\n",
    "# Filter for valid TFs\n",
    "f2m = {k:v for k,v in f2m.items() if k in valid_factors}\n",
    "\n",
    "# Only use TFs for which we have data\n",
    "factors = list(set([x.split(\".\")[0].split(\"/\")[-1] for x in glob(f\"{out_overlap}/*enhancers.txt\")]))\n",
    "valid_factors = [f for f in factors if f in valid_factors]\n",
    "\n",
    "print(len(valid_factors), \"factors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one big dataframe with all TF peak overlap with reference enhancer set\n",
    "\n",
    "fnames = glob(f\"{train_dir}/peaks/*narrowPeak\")\n",
    "fnames = [fname for fname in fnames if re.search(\"|\".join([x + '[.]' for x in cell_types]), fname)]\n",
    "\n",
    "y_true_file = f\"{train_dir}/all_tfs_y_true.feather\"\n",
    "\n",
    "if force_rerun or not os.path.exists(y_true_file):\n",
    "    y_true = pd.DataFrame()\n",
    "    for fname in tqdm(fnames):\n",
    "        m = re.search(r\"(\\w+)\\.([\\w-]+)\\..*\", fname)\n",
    "        factor = m.group(1)\n",
    "        cell_type = m.group(2)\n",
    "        if factor not in factors:\n",
    "            continue        \n",
    "        try:\n",
    "            y_true[f\"{factor}.{cell_type}\"] = pd.read_table(f\"{out_overlap}/{factor}.{cell_type}.enhancers.txt\", \n",
    "                                                                index_col=0, names=[f\"{factor}.{cell_type}\"]).iloc[:,0]\n",
    "        except Exception as e:\n",
    "            print(fname, e)\n",
    "    y_true.reset_index().to_feather(y_true_file)\n",
    "else:\n",
    "    logger.debug(\"reading y_true\")\n",
    "    y_true = pd.read_feather(f\"{train_dir}/all_tfs_y_true.feather\")\n",
    "    y_true = y_true.set_index(y_true.columns[0])\n",
    "    \n",
    "y_true[y_true > 1] = 1  # peak, yes or no\n",
    "\n",
    "for column in y_true: # Remove TFs that have 0 true peaks\n",
    "    if (y_true[column] == 1).sum() == 0:\n",
    "        print(column)\n",
    "        print(y_true[column].sum())\n",
    "        y_true.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remap coverage\n",
    "\n",
    "This represents the average binding of TFs across cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remap_file = f\"{out_dir}/reference.coverage.txt\"\n",
    "if force_rerun or not os.path.exists(remap_file):\n",
    "    !coverage_table -p {ref_bed} -d {coverage_bw} > {remap_file}\n",
    "remap_cov = pd.read_table(remap_file, sep=\"\\t\", comment=\"#\", index_col=0)\n",
    "remap_cov.rename(columns={remap_cov.columns[0]:\"average\"}, inplace=True)\n",
    "remap_cov[\"average\"] = remap_cov[\"average\"] / remap_cov[\"average\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancer data\n",
    "\n",
    "Transform and normalize CAGE enhancer expression data (TPMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.log1p(data)\n",
    "data = qnorm.quantile_normalize(data)\n",
    "data.loc[:,:] = minmax_scale(data)\n",
    "\n",
    "data.columns = data.columns + \".CAGE\"\n",
    "tables = {\n",
    "    \"CAGE\":data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create base files for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = remap_cov\n",
    "\n",
    "# Big motif scan table\n",
    "gimme = pd.read_table(motif_scan_file, index_col=0, comment=\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First pass for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(sorted(cell_types))\n",
    "print(sorted(tables[\"CAGE\"].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    (\"motif\", \"CAGE\"),\n",
    "    (\"average\", \"CAGE\"),\n",
    "    (\"average\", \"motif\", \"CAGE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_benchmark = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = y_true.columns.str.replace(\"\\..*\", \"\").value_counts()\n",
    "factors = factors[factors > 1].index\n",
    "\n",
    "test_idx = y_true.index[y_true.index.str.contains(test_chrom_regex)]\n",
    "train_idx = y_true.index[~y_true.index.str.contains(test_chrom_regex)]\n",
    "\n",
    "marks = list(tables.keys())\n",
    "meanref = {mark:tables[mark].mean(1) for mark in marks}\n",
    "common_f = [f for f in factors if f in valid_factors]\n",
    "for factor in common_f:\n",
    "    \n",
    "    if factor not in valid_factors or factor not in f2m:\n",
    "        logger.debug(f\"Skipping {factor}, not a TF or no motif known\")\n",
    "        continue\n",
    "    if factor in set([x[0] for x in ct_benchmark]):\n",
    "        logger.debug(f\"Skipping {factor}, already done\")\n",
    "        continue\n",
    "    logger.info(f\"Model benchmark: {factor}\")\n",
    "    \n",
    "    cols = y_true.columns[y_true.columns.str.contains(f\"^{factor}\\.\")]\n",
    "    cell_types = [col.split(\".\")[-1] for col in cols]\n",
    "    \n",
    "    cell_types = [ct for ct in cell_types if ct != \"LNCaP\"]\n",
    "    print(cell_types)\n",
    "    if len(cell_types) == 1:\n",
    "        # Can't check in other cell-types\n",
    "        continue\n",
    "    motif_frame = gimme[f2m[factor]].mean(1).to_frame(\"motif\")\n",
    "    X = X_base.join(motif_frame)\n",
    "    #X = X_base\n",
    "    \n",
    "    tmp = pd.DataFrame()\n",
    "    for ct in cell_types:\n",
    "        a = [tables[mark][[f\"{ct}.{mark}\"]] for mark in marks] \n",
    "        #b = [tables[mark][[f\"{ct}.{mark}\"]].sub(meanref[mark], axis=0).rename(columns={f\"{ct}.{mark}\":f\"{ct}.{mark}.relative\"}) for mark in [\"ATAC\"]]\n",
    "        ct_frame = pd.concat(a  + [y_true[f\"{factor}.{ct}\"].rename(\"y_true\")], axis=1)\n",
    "        ct_frame.columns = ct_frame.columns.str.replace(f\"{ct}.\", \"\")\n",
    "        ct_frame[\"cell_type\"] = ct\n",
    "        tmp = pd.concat([tmp, X.join(ct_frame)])\n",
    "    \n",
    "    X = tmp\n",
    "    for test_cell in cell_types:\n",
    "        print(test_cell)\n",
    "        train_cells = [c for c in cell_types if c != test_cell]\n",
    "        \n",
    "        train = X.loc[X.index.intersection(train_idx),:]\n",
    "        test = X.loc[X.index.intersection(test_idx),:]\n",
    "\n",
    "        X_train = train[train[\"cell_type\"] != test_cell].drop(columns=[\"cell_type\"])\n",
    "        X_test = test[test[\"cell_type\"] == test_cell].drop(columns=[\"cell_type\"])\n",
    "        logger.debug(f\"X_train: {str(X_train.shape)}\")\n",
    "        \n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        if X_train[X_train[\"y_true\"] == 0].shape[0] >= 100000:\n",
    "            X_train = pd.concat((X_train[X_train[\"y_true\"] == 1], X_train[X_train[\"y_true\"] == 0].sample(100000)))\n",
    "        y_train = X_train[[\"y_true\"]]\n",
    "        y_test = X_test[[\"y_true\"]]\n",
    "        \n",
    "\n",
    "#         print(y_train)\n",
    "#         print(y_test)\n",
    "        \n",
    "        if y_train.sum()[0] < 50 or y_test.sum()[0] < 50:\n",
    "            print(\"skipping, not enough peaks\")\n",
    "            continue\n",
    "        \n",
    "        X_train = X_train.drop(columns=[\"y_true\"])\n",
    "        X_test = X_test.drop(columns=[\"y_true\"])\n",
    "        \n",
    "\n",
    "#         print(X_train.sample(10))\n",
    "        #print(X_test.sample(10))\n",
    "        \n",
    "\n",
    "        print(f\"{factor}\\t{test_cell}\\tFitting models...\")\n",
    "        print(f\"{factor}\\t{test_cell}\\tbaseline\\tpr_auc\\t{y_test.mean()[0]:.3f}\")\n",
    "        ct_benchmark.append([factor, test_cell, \"baseline\", \"pr_auc\", y_test.mean()[0]])\n",
    "        for param_set in params:\n",
    "            param_columns = sorted(param_set)\n",
    "            model_name = \"_\".join(param_columns)\n",
    "            model.fit(X_train[param_columns], y_train)\n",
    "            y_pred = model.predict_proba(X_test[param_columns])[:,1]\n",
    "            for name, func in scores.items():\n",
    "                score = func(y_test, y_pred)\n",
    "                ct_benchmark.append([factor, test_cell, model_name, name, score])\n",
    "                print(f\"{factor}\\t{test_cell}\\t{model_name}\\t{name}\\t{score:.3f}\")\n",
    "\n",
    "        # score baselines\n",
    "        for base in [\"CAGE\", \"average\", \"motif\"]:\n",
    "            for name, func in scores.items():\n",
    "                score = func(y_test, X_test[base])\n",
    "                ct_benchmark.append([factor, test_cell, f\"{base}.baseline\", name, score])\n",
    "                if base in [\"average\", \"CAGE\"]:\n",
    "                    # Relevant baseline: average binding across cell types\n",
    "                    print(f\"{factor}\\t{test_cell}\\t{base}.baseline\\t{name}\\t{score:.3f}\")        \n",
    "    \n",
    "    ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "\n",
    "    ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"pr_auc\"].median()\n",
    "ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark_pr_fp.txt\", sep=\"\\t\")\n",
    "\n",
    "order = ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"pr_auc\"].groupby([\"model\", \"score\"]).median().reset_index().sort_values(\"value\")[\"model\"].values\n",
    "sns.boxplot(data=ct_benchmark_lr[ct_benchmark_lr[\"score\"]==\"pr_auc\"], x=\"value\", y=\"model\", order=order)\n",
    "plt.xlabel(\"Precision-Recall AUC\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PR_AUC_first_pass_Benchmark.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"roc_auc\"].median()\n",
    "ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark_roc_fp.txt\", sep=\"\\t\")\n",
    "\n",
    "order = ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"roc_auc\"].groupby([\"model\", \"score\"]).median().reset_index().sort_values(\"value\")[\"model\"].values\n",
    "sns.boxplot(data=ct_benchmark_lr[ct_benchmark_lr[\"score\"]==\"roc_auc\"], x=\"value\", y=\"model\", order=order)\n",
    "plt.xlabel(\"Receiver-operator characteristic (ROC) AUC\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ROC_AUC_first_pass_Benchmark.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second pass: training full models\n",
    "\n",
    "Trained models are saved as pickles using joblib. Not sure if this is the most safe / optimal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "factors = y_true.columns.str.replace(\"\\..*\", \"\").value_counts()\n",
    "factors = factors.index\n",
    "\n",
    "\n",
    "X_general = pd.DataFrame()\n",
    "\n",
    "for factor in factors:\n",
    "    if factor not in valid_factors or factor not in f2m:\n",
    "        logger.debug(f\"Skipping {factor}, not a TF or no motif known\")\n",
    "        continue\n",
    "    print(factor)\n",
    "    cols = y_true.columns[y_true.columns.str.contains(f\"^{factor}\\.\")]\n",
    "    cell_types = [col.split(\".\")[-1] for col in cols]\n",
    "    cell_types = [ct for ct in cell_types if ct != \"LNCaP\"]\n",
    "    #cell_types = [ct for ct in cell_types if ct != \"MCF-7\"]\n",
    "    \n",
    "    motif_frame = gimme[f2m[factor]].mean(1).to_frame(\"motif\")\n",
    "    X = X_base.join(motif_frame)\n",
    "    \n",
    "    tmp = pd.DataFrame()\n",
    "    for ct in cell_types:         \n",
    "#         a = [tables[mark][[f\"{ct}.{mark}\"]].sub(meanref[mark], axis=0).apply(scale).rename(columns={f\"{ct}.{mark}\":f\"{ct}.{mark}.relative\"}) for mark in [\"ATAC\"]]\n",
    "        b = [tables[mark][[f\"{ct}.{mark}\"]] for mark in marks] \n",
    "\n",
    "        ct_frame = pd.concat(b + [y_true[f\"{factor}.{ct}\"].rename(\"y_true\")], axis=1)\n",
    "        ct_frame.columns = ct_frame.columns.str.replace(f\"{ct}.\", \"\")\n",
    "        ct_frame[\"cell_type\"] = ct\n",
    "        tmp = pd.concat([tmp, X.join(ct_frame)])\n",
    "        #print(tmp.head())\n",
    "    \n",
    "    X = tmp\n",
    "    if not \"y_true\" in X.columns:\n",
    "        continue\n",
    "    # Use all positive regions, and randomly sample negative regions\n",
    "    # Initially 100,000 random negative regions were sampled, but I changed it to randomly sample \n",
    "    # total rows - all true positives \n",
    "    X = pd.concat((X[X[\"y_true\"] == 1], X[X[\"y_true\"] == 0].sample((len(X) - (X.y_true == 1).sum()))))\n",
    "    \n",
    "    X_general = pd.concat((X_general, X.sample(5000))) # Use sample for general model\n",
    "    y = X[[\"y_true\"]]\n",
    "    X = X.drop(columns=[\"y_true\"])\n",
    "    X = X.rename(columns={\"remap.w50\":\"average\"})\n",
    "    \n",
    "    for param_set in params:\n",
    "        print(f\"{factor}\\tFitting model...\")\n",
    "        \n",
    "        model.fit(X[sorted(param_set)], y)\n",
    "        dirname = os.path.join(out_dir, \"_\".join(sorted(param_set)))\n",
    "        fname = os.path.join(dirname, f\"{factor}.pkl\"  )\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        joblib.dump(model, fname)\n",
    "\n",
    "y = X_general[[\"y_true\"]]\n",
    "X = X_general.drop(columns=[\"y_true\"])\n",
    "X = X.rename(columns={\"remap.w50\":\"average\"})\n",
    "\n",
    "\n",
    "# Make a general model, that is not TF specific\n",
    "for param_set in params:\n",
    "    print(f\"Fitting general model...\")\n",
    "    #print(X.head())\n",
    "    model.fit(X[sorted(param_set)], y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dirname = \"_\".join(sorted(param_set))\n",
    "    fname = os.path.join(out_dir, dirname, \"general.pkl\"  )\n",
    "#     if not os.path.exists(dirname):\n",
    "#         os.makedirs(dirname)\n",
    "    joblib.dump(model, fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third part: benchmarking the trained general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = y_true.columns.str.replace(\"\\..*\", \"\").value_counts()\n",
    "factors = factors[factors > 1].index\n",
    "\n",
    "test_idx = y_true.index[y_true.index.str.contains(test_chrom_regex)]\n",
    "train_idx = y_true.index[~y_true.index.str.contains(test_chrom_regex)]\n",
    "\n",
    "marks = list(tables.keys())\n",
    "meanref = {mark:tables[mark].mean(1) for mark in marks}\n",
    "common_f = [f for f in factors if f in valid_factors]\n",
    "for factor in common_f:\n",
    "    \n",
    "    if factor not in valid_factors or factor not in f2m:\n",
    "        logger.debug(f\"Skipping {factor}, not a TF or no motif known\")\n",
    "        continue\n",
    "#     if factor in set([x[0] for x in ct_benchmark]):\n",
    "#         logger.debug(f\"Skipping {factor}, already done\")\n",
    "#         continue\n",
    "    logger.info(f\"Model benchmark: {factor}\")\n",
    "    \n",
    "    cols = y_true.columns[y_true.columns.str.contains(f\"^{factor}\\.\")]\n",
    "    cell_types = [col.split(\".\")[-1] for col in cols]\n",
    "    \n",
    "    cell_types = [ct for ct in cell_types if ct != \"LNCaP\"]\n",
    "    print(cell_types)\n",
    "    if len(cell_types) == 1:\n",
    "        # Can't check in other cell-types\n",
    "        continue\n",
    "    motif_frame = gimme[f2m[factor]].mean(1).to_frame(\"motif\")\n",
    "    X = X_base.join(motif_frame)\n",
    "    \n",
    "    tmp = pd.DataFrame()\n",
    "    for ct in cell_types:\n",
    "        a = [tables[mark][[f\"{ct}.{mark}\"]] for mark in marks] \n",
    "#         b = [tables[mark][[f\"{ct}.{mark}\"]].sub(meanref[mark], axis=0).rename(columns={f\"{ct}.{mark}\":f\"{ct}.{mark}.relative\"}) for mark in [\"ATAC\"]]\n",
    "        ct_frame = pd.concat(a + [y_true[f\"{factor}.{ct}\"].rename(\"y_true\")], axis=1)\n",
    "        ct_frame.columns = ct_frame.columns.str.replace(f\"{ct}.\", \"\")\n",
    "        ct_frame[\"cell_type\"] = ct\n",
    "        tmp = pd.concat([tmp, X.join(ct_frame)])\n",
    "    \n",
    "    X = tmp\n",
    "    for test_cell in cell_types:\n",
    "        test = X.loc[test_idx]\n",
    "        X_test = test[test[\"cell_type\"] == test_cell].drop(columns=[\"cell_type\"])\n",
    "        y_test = X_test[[\"y_true\"]]\n",
    "        \n",
    "        if y_test.sum()[0] < 50:\n",
    "            print(\"skipping, not enough peaks\")\n",
    "            continue\n",
    "        \n",
    "        X_test = X_test.drop(columns=[\"y_true\"])\n",
    "        \n",
    "        for param_set in params:\n",
    "            param_columns = sorted(param_set)\n",
    "            model_name = \"_\".join(param_columns)\n",
    "            \n",
    "            dirname = \"_\".join(sorted(param_set))\n",
    "            fname = os.path.join(out_dir, dirname, \"general.pkl\"  )\n",
    "            model = joblib.load(fname)\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test[param_columns])[:,1]\n",
    "            for name, func in scores.items():\n",
    "                score = func(y_test, y_pred)\n",
    "                ct_benchmark.append([factor, test_cell, f\"{model_name}.general\", name, score])\n",
    "                print(f\"{factor}\\t{test_cell}\\t{model_name}.general\\t{name}\\t{score:.3f}\")\n",
    "    ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "    ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark.with_general.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"pr_auc\"].median()\n",
    "ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark_pr_gm.txt\", sep=\"\\t\")\n",
    "\n",
    "order = ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"pr_auc\"].groupby([\"model\", \"score\"]).median().reset_index().sort_values(\"value\")[\"model\"].values\n",
    "sns.boxplot(data=ct_benchmark_lr[ct_benchmark_lr[\"score\"]==\"pr_auc\"], x=\"value\", y=\"model\", order=order)\n",
    "plt.xlabel(\"Precision-Recall AUC\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PR_AUC_full_Benchmark.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_benchmark_lr = pd.DataFrame(ct_benchmark, columns=[\"factor\", \"test_cell_type\", \"model\", \"score\", \"value\"])\n",
    "ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"roc_auc\"].median()\n",
    "ct_benchmark_lr.to_csv(f\"{out_dir}/benchmark_roc_gm.txt\", sep=\"\\t\")\n",
    "\n",
    "order = ct_benchmark_lr[ct_benchmark_lr[\"score\"] == \"roc_auc\"].groupby([\"model\", \"score\"]).median().reset_index().sort_values(\"value\")[\"model\"].values\n",
    "sns.boxplot(data=ct_benchmark_lr[ct_benchmark_lr[\"score\"]==\"roc_auc\"], x=\"value\", y=\"model\", order=order)\n",
    "plt.xlabel(\"Receiver-operator characteristic (ROC) AUC\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ROC_AUC_full_Benchmark.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
